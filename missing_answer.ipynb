{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de054f65-91ca-4c3c-b76b-7ead57ac8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from nltk import sent_tokenize\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import random\n",
    "from transformers import AutoTokenizer, DPRQuestionEncoder,DPRContextEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  6 16:42:34 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   54C    P0             227W / 400W |  34321MiB / 81920MiB |     83%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   31C    P0              60W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   66C    P0             313W / 400W |  12917MiB / 81920MiB |     96%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   59C    P0             283W / 400W |   8391MiB / 81920MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    939160      C   ...e/jeongseokoh/miniconda3/bin/python    34308MiB |\n",
      "|    2   N/A  N/A    952319      C   python                                    12904MiB |\n",
      "|    3   N/A  N/A    952319      C   python                                     8378MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7d2e9a-4c54-47a7-b32c-2505db1518df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"Seongill/nq_squad\")[\"train\"]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 97888\n",
      "After: 97888\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", len(df))\n",
    "df = df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
    "print(\"After:\" , len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fc03f0c71c423fb66bdc7ace784706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Question Embedding...:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05b71db946b465baa5c53496bfc49ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d39b3110e634dcaa2195739c0990f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4a63ca281d4e3a85bdbdc5e575762d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee1d02369ef4bd682261ecd99619e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543378fd44174a09a674d0875d097400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cd53aabf6b4d76920176469e2a1f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Context Embedding...:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/seongilpark/rag/missing_answer.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B114.110.134.186/home/seongilpark/rag/missing_answer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m ctx \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(ctxs), \u001b[39m1024\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mContext Embedding...\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B114.110.134.186/home/seongilpark/rag/missing_answer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     batch \u001b[39m=\u001b[39m ctxs[i:i\u001b[39m+\u001b[39m\u001b[39m1028\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B114.110.134.186/home/seongilpark/rag/missing_answer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     output \u001b[39m=\u001b[39m tokenizer(batch, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B114.110.134.186/home/seongilpark/rag/missing_answer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B114.110.134.186/home/seongilpark/rag/missing_answer.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moutput)\u001b[39m.\u001b[39mpooler_output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39m# [args.batch_size, hidden_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/exp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/exp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[1;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2887\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2888\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2889\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2890\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2891\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2892\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2893\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2894\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2895\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2896\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2897\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2898\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2899\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2900\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2901\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/exp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/exp/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:537\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    536\u001b[0m sanitized_tokens \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 537\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m tokens_and_encodings[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    538\u001b[0m     stack \u001b[39m=\u001b[39m [e \u001b[39mfor\u001b[39;00m item, _ \u001b[39min\u001b[39;00m tokens_and_encodings \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m item[key]]\n\u001b[1;32m    539\u001b[0m     sanitized_tokens[key] \u001b[39m=\u001b[39m stack\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ctxs = list(set(df[\"context\"].tolist()))\n",
    "questions = df[\"question\"].tolist()\n",
    "questions_embed, ctxs_embed = [], []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(\"cuda\")\n",
    "\n",
    "for i in tqdm(range(0, len(questions), 1028), desc=\"Question Embedding...\"):\n",
    "    batch = questions[i:i+1028]\n",
    "    output = tokenizer(batch, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**output).pooler_output.detach().cpu().numpy() # [args.batch_size, hidden_dim]\n",
    "    questions_embed.extend([emb for emb in embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757f436f47a04b4c90d7693df7433298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Context Embedding...:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctxs_embed = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(\"cuda\")\n",
    "for i in tqdm(range(0, len(ctxs), 1024), desc=\"Context Embedding...\"):\n",
    "    batch = ctxs[i:i+1024]\n",
    "    output = tokenizer(batch, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**output).pooler_output.detach().cpu().numpy() # [args.batch_size, hidden_dim]\n",
    "    ctxs_embed.extend([emb for emb in embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_emb = {}\n",
    "for k, v in zip(ctxs, ctxs_embed):\n",
    "    text_to_emb[k] = v / np.linalg.norm(v)\n",
    "norm_ctxs_embed = np.array(ctxs_embed) / np.linalg.norm(ctxs_embed, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_questions_embed = np.array(questions_embed) / np.linalg.norm(questions_embed, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_to_emb = {}\n",
    "for k, v in zip(questions, questions_embed):\n",
    "    q_to_emb[k] = v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [d[\"text\"][0] for d in df[\"answers\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What conjunction would be used to join a person's surnames? in\n",
      "n n\n",
      "What letter represents the index of refraction? n\n"
     ]
    }
   ],
   "source": [
    "for q, a in zip(questions, answer):\n",
    "    if a == \"n\":\n",
    "        print(q, a)\n",
    "    if a == \"in\":\n",
    "        print(q, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select(text, answer):\n",
    "    item = random.choice(ctxs)\n",
    "    cnt = 0\n",
    "    if answer in [\"n\", \"in\", \"\"]:\n",
    "        return None\n",
    "    while (item == text) or (answer in item):\n",
    "        item = random.choice(ctxs)\n",
    "        cnt += 1\n",
    "        if cnt > 100:\n",
    "            return None\n",
    "    return item\n",
    "def similarity_select(text, answer, question):\n",
    "    c_emb = text_to_emb[text]\n",
    "    q_emb = q_to_emb[question]\n",
    "    c_output = np.matmul(norm_ctxs_embed, c_emb.T)\n",
    "    q_output = np.matmul(norm_ctxs_embed, q_emb.T)\n",
    "    mean_output = (c_output + q_output) / 2\n",
    "    topk_idx = list(np.argpartition(mean_output, -20)[-20:])\n",
    "    for idx in topk_idx:\n",
    "        if answer not in ctxs[idx]:\n",
    "            return ctxs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af1cedc189c4d24b1fc7971ccbb7692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_answer, similar_answer = [], []\n",
    "for row in tqdm(df.iterrows()):\n",
    "    text = row[1][\"context\"]\n",
    "    answer = row[1][\"answers\"][\"text\"][0]\n",
    "    question = row[1][\"question\"]\n",
    "    random_answer.append(random_select(text, answer))\n",
    "    similar_answer.append(similarity_select(text, answer,question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_944773/367060725.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"similar_answer_v2\"] = similar_answer\n"
     ]
    }
   ],
   "source": [
    "#df[\"random_answer\"] = random_answer\n",
    "df[\"similar_answer_v2\"] = similar_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>masked_query</th>\n",
       "      <th>query_embedding</th>\n",
       "      <th>random_answer</th>\n",
       "      <th>similar_answer</th>\n",
       "      <th>similar_answer_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[-0.04606574401259422, 0.015957484021782875, -...</td>\n",
       "      <td>Before the St. Elizabeth's flood (1421), the M...</td>\n",
       "      <td>In 2014 the Notre Dame student body consisted ...</td>\n",
       "      <td>The doctrines of the Assumption or Dormition o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "      <td>What is in front of [MASK]?</td>\n",
       "      <td>[0.19923186302185059, 0.06610594689846039, 0.2...</td>\n",
       "      <td>The government broadened land ownership by ret...</td>\n",
       "      <td>In 2014 the Notre Dame student body consisted ...</td>\n",
       "      <td>St. Patrick's Street, the main street of the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "      <td>[MASK] at [MASK] is beside to which structure?</td>\n",
       "      <td>[0.3204971253871918, 0.20272424817085266, 0.22...</td>\n",
       "      <td>In the 1520s during the Protestant Reformation...</td>\n",
       "      <td>In 2014 the Notre Dame student body consisted ...</td>\n",
       "      <td>One of the most dramatic parts of the museum i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "      <td>What is the Grotto at [MASK]?</td>\n",
       "      <td>[0.020319189876317978, 0.21503326296806335, 0....</td>\n",
       "      <td>Domestic dogs inherited complex behaviors, suc...</td>\n",
       "      <td>In 2014 the Notre Dame student body consisted ...</td>\n",
       "      <td>The structure known as \"Virgil's tomb\" is foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "      <td>What sits on top of [MASK] at [MASK]?</td>\n",
       "      <td>[0.21866606175899506, 0.32595109939575195, 0.0...</td>\n",
       "      <td>In modern-day Germany, the Holy Roman Empire c...</td>\n",
       "      <td>In 2014 the Notre Dame student body consisted ...</td>\n",
       "      <td>In the 18 years under the presidency of Edward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98164</th>\n",
       "      <td>5737aafd1c456719005744fb</td>\n",
       "      <td>Force</td>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What is the metric term less used than the New...</td>\n",
       "      <td>{'text': ['kilogram-force', 'pound-force', 'ki...</td>\n",
       "      <td>What is the metric term less used than the [MA...</td>\n",
       "      <td>[0.05857216566801071, 0.13753995299339294, -0....</td>\n",
       "      <td>The stated objective of most intellectual prop...</td>\n",
       "      <td>A unit load is defined as 100 mA in USB 1.x an...</td>\n",
       "      <td>Since forces are perceived as pushes or pulls,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98165</th>\n",
       "      <td>5737aafd1c456719005744fc</td>\n",
       "      <td>Force</td>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What is the kilogram-force sometimes reffered ...</td>\n",
       "      <td>{'text': ['kilopond', 'kilopond', 'kilopond', ...</td>\n",
       "      <td>What is the kilogram-force sometimes reffered ...</td>\n",
       "      <td>[0.13551265001296997, 0.04391534999012947, 0.1...</td>\n",
       "      <td>The serial format changed for the 2005 revival...</td>\n",
       "      <td>A unit load is defined as 100 mA in USB 1.x an...</td>\n",
       "      <td>The total energy of a system can be subdivided...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98166</th>\n",
       "      <td>5737aafd1c456719005744fd</td>\n",
       "      <td>Force</td>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What is a very seldom used unit of mass in the...</td>\n",
       "      <td>{'text': ['slug', 'metric slug', 'metric slug'...</td>\n",
       "      <td>What is a very seldom used unit of mass in the...</td>\n",
       "      <td>[0.13060449063777924, 0.13672053813934326, -0....</td>\n",
       "      <td>Each cardinal takes on a titular church, eithe...</td>\n",
       "      <td>A unit load is defined as 100 mA in USB 1.x an...</td>\n",
       "      <td>A watt balance is an instrument for comparing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98167</th>\n",
       "      <td>5737aafd1c456719005744fe</td>\n",
       "      <td>Force</td>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What seldom used term of a unit of force equal...</td>\n",
       "      <td>{'text': ['kip', 'kip', 'kip', 'kip', 'kip'], ...</td>\n",
       "      <td>What seldom used term of a unit of force equal...</td>\n",
       "      <td>[-0.10803636163473129, 0.0015136328293010592, ...</td>\n",
       "      <td>President Franklin D. Roosevelt promoted a \"go...</td>\n",
       "      <td>A unit load is defined as 100 mA in USB 1.x an...</td>\n",
       "      <td>A static equilibrium between two forces is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98168</th>\n",
       "      <td>5737aafd1c456719005744ff</td>\n",
       "      <td>Force</td>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What is the seldom used force unit equal to on...</td>\n",
       "      <td>{'text': ['sthène', 'sthène', 'sthène', 'sthèn...</td>\n",
       "      <td>What is the seldom used force unit equal to [M...</td>\n",
       "      <td>[-0.049544744193553925, -0.0007250725757330656...</td>\n",
       "      <td>Starting in 1910, the army began acquiring fix...</td>\n",
       "      <td>A unit load is defined as 100 mA in USB 1.x an...</td>\n",
       "      <td>In the late 17th century, Gottfried Leibniz pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97748 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id                     title  \\\n",
       "0      5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1      5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2      5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3      5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4      5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "...                         ...                       ...   \n",
       "98164  5737aafd1c456719005744fb                     Force   \n",
       "98165  5737aafd1c456719005744fc                     Force   \n",
       "98166  5737aafd1c456719005744fd                     Force   \n",
       "98167  5737aafd1c456719005744fe                     Force   \n",
       "98168  5737aafd1c456719005744ff                     Force   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Architecturally, the school has a Catholic cha...   \n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "98164  The pound-force has a metric counterpart, less...   \n",
       "98165  The pound-force has a metric counterpart, less...   \n",
       "98166  The pound-force has a metric counterpart, less...   \n",
       "98167  The pound-force has a metric counterpart, less...   \n",
       "98168  The pound-force has a metric counterpart, less...   \n",
       "\n",
       "                                                question  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...   \n",
       "1      What is in front of the Notre Dame Main Building?   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                      What is the Grotto at Notre Dame?   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "98164  What is the metric term less used than the New...   \n",
       "98165  What is the kilogram-force sometimes reffered ...   \n",
       "98166  What is a very seldom used unit of mass in the...   \n",
       "98167  What seldom used term of a unit of force equal...   \n",
       "98168  What is the seldom used force unit equal to on...   \n",
       "\n",
       "                                                 answers  \\\n",
       "0      {'text': ['Saint Bernadette Soubirous'], 'answ...   \n",
       "1      {'text': ['a copper statue of Christ'], 'answe...   \n",
       "2      {'text': ['the Main Building'], 'answer_start'...   \n",
       "3      {'text': ['a Marian place of prayer and reflec...   \n",
       "4      {'text': ['a golden statue of the Virgin Mary'...   \n",
       "...                                                  ...   \n",
       "98164  {'text': ['kilogram-force', 'pound-force', 'ki...   \n",
       "98165  {'text': ['kilopond', 'kilopond', 'kilopond', ...   \n",
       "98166  {'text': ['slug', 'metric slug', 'metric slug'...   \n",
       "98167  {'text': ['kip', 'kip', 'kip', 'kip', 'kip'], ...   \n",
       "98168  {'text': ['sthène', 'sthène', 'sthène', 'sthèn...   \n",
       "\n",
       "                                            masked_query  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...   \n",
       "1                            What is in front of [MASK]?   \n",
       "2         [MASK] at [MASK] is beside to which structure?   \n",
       "3                          What is the Grotto at [MASK]?   \n",
       "4                  What sits on top of [MASK] at [MASK]?   \n",
       "...                                                  ...   \n",
       "98164  What is the metric term less used than the [MA...   \n",
       "98165  What is the kilogram-force sometimes reffered ...   \n",
       "98166  What is a very seldom used unit of mass in the...   \n",
       "98167  What seldom used term of a unit of force equal...   \n",
       "98168  What is the seldom used force unit equal to [M...   \n",
       "\n",
       "                                         query_embedding  \\\n",
       "0      [-0.04606574401259422, 0.015957484021782875, -...   \n",
       "1      [0.19923186302185059, 0.06610594689846039, 0.2...   \n",
       "2      [0.3204971253871918, 0.20272424817085266, 0.22...   \n",
       "3      [0.020319189876317978, 0.21503326296806335, 0....   \n",
       "4      [0.21866606175899506, 0.32595109939575195, 0.0...   \n",
       "...                                                  ...   \n",
       "98164  [0.05857216566801071, 0.13753995299339294, -0....   \n",
       "98165  [0.13551265001296997, 0.04391534999012947, 0.1...   \n",
       "98166  [0.13060449063777924, 0.13672053813934326, -0....   \n",
       "98167  [-0.10803636163473129, 0.0015136328293010592, ...   \n",
       "98168  [-0.049544744193553925, -0.0007250725757330656...   \n",
       "\n",
       "                                           random_answer  \\\n",
       "0      Before the St. Elizabeth's flood (1421), the M...   \n",
       "1      The government broadened land ownership by ret...   \n",
       "2      In the 1520s during the Protestant Reformation...   \n",
       "3      Domestic dogs inherited complex behaviors, suc...   \n",
       "4      In modern-day Germany, the Holy Roman Empire c...   \n",
       "...                                                  ...   \n",
       "98164  The stated objective of most intellectual prop...   \n",
       "98165  The serial format changed for the 2005 revival...   \n",
       "98166  Each cardinal takes on a titular church, eithe...   \n",
       "98167  President Franklin D. Roosevelt promoted a \"go...   \n",
       "98168  Starting in 1910, the army began acquiring fix...   \n",
       "\n",
       "                                          similar_answer  \\\n",
       "0      In 2014 the Notre Dame student body consisted ...   \n",
       "1      In 2014 the Notre Dame student body consisted ...   \n",
       "2      In 2014 the Notre Dame student body consisted ...   \n",
       "3      In 2014 the Notre Dame student body consisted ...   \n",
       "4      In 2014 the Notre Dame student body consisted ...   \n",
       "...                                                  ...   \n",
       "98164  A unit load is defined as 100 mA in USB 1.x an...   \n",
       "98165  A unit load is defined as 100 mA in USB 1.x an...   \n",
       "98166  A unit load is defined as 100 mA in USB 1.x an...   \n",
       "98167  A unit load is defined as 100 mA in USB 1.x an...   \n",
       "98168  A unit load is defined as 100 mA in USB 1.x an...   \n",
       "\n",
       "                                       similar_answer_v2  \n",
       "0      The doctrines of the Assumption or Dormition o...  \n",
       "1      St. Patrick's Street, the main street of the c...  \n",
       "2      One of the most dramatic parts of the museum i...  \n",
       "3      The structure known as \"Virgil's tomb\" is foun...  \n",
       "4      In the 18 years under the presidency of Edward...  \n",
       "...                                                  ...  \n",
       "98164  Since forces are perceived as pushes or pulls,...  \n",
       "98165  The total energy of a system can be subdivided...  \n",
       "98166  A watt balance is an instrument for comparing ...  \n",
       "98167  A static equilibrium between two forces is the...  \n",
       "98168  In the late 17th century, Gottfried Leibniz pr...  \n",
       "\n",
       "[97748 rows x 10 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop([\"random_context\", \"similarity_context\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"answer\"] = df[\"answers\"].apply(lambda x: x[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd91b8a3-04bb-426e-a358-85dad1a30e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642931eccb1842ab9e18d5b9783de9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c2bea6c3404674b8193aeea3df3559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097cad07d894434d84c662bf83ace22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.push_to_hub(\"squad_missing_answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204cd825-6579-4bf4-90c9-684cc658597e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', 'As', ' of', ' January', ' ', '202', '2', ',', ' the', ' president']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.choices[0].logprobs.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7c381d-a895-4cb9-b30c-85109e9a64c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'\\n\\n': -0.11258054,\n",
       "  '\\n': -2.5332043,\n",
       "  ' \\n\\n': -4.746234,\n",
       "  'The': -5.9145384,\n",
       "  '\\n\\n\\n': -6.4242067},\n",
       " {'As': -0.042069715,\n",
       "  'The': -3.47413,\n",
       "  'Currently': -5.265245,\n",
       "  'Joe': -5.463491,\n",
       "  'Donald': -8.244761},\n",
       " {' of': -0.009028853,\n",
       "  ' an': -5.287681,\n",
       "  ' a': -5.7988505,\n",
       "  ' the': -8.499787,\n",
       "  ' this': -8.600926},\n",
       " {' January': -0.78186524,\n",
       "  ' ': -1.2735778,\n",
       "  ' November': -2.7348604,\n",
       "  ' October': -2.9311075,\n",
       "  ' February': -3.7668085},\n",
       " {' ': -6.80205e-05,\n",
       "  ',': -10.268138,\n",
       "  ' of': -11.059773,\n",
       "  '<|endoftext|>': -11.653062,\n",
       "  '202': -12.60736},\n",
       " {'202': -0.08436187,\n",
       "  '20': -2.5598962,\n",
       "  '21': -6.6945915,\n",
       "  '11': -8.0714855,\n",
       "  '201': -8.331005},\n",
       " {'2': -0.5250918,\n",
       "  '1': -0.90931386,\n",
       "  '0': -5.188664,\n",
       "  '3': -9.381581,\n",
       "  '<|endoftext|>': -11.086154},\n",
       " {',': -0.00012296606,\n",
       "  ' the': -10.333541,\n",
       "  ',the': -10.737315,\n",
       "  '.': -11.466929,\n",
       "  '<|endoftext|>': -11.554799},\n",
       " {' the': -0.11537982,\n",
       "  ' Joe': -2.236422,\n",
       "  ' Joseph': -6.6280236,\n",
       "  ' President': -7.9406114,\n",
       "  ' The': -8.514186},\n",
       " {' president': -0.21514872,\n",
       "  ' President': -2.2915952,\n",
       "  ' current': -2.3906076,\n",
       "  ' ': -7.464636,\n",
       "  ' incumbent': -9.844528}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.choices[0].logprobs.top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '100303db73e4051089035f246d0aeef2b12c4e47',\n",
       " 'title': 'Newcastle_upon_Tyne',\n",
       " 'context': \"Another green space in Newcastle is the Town Moor, lying immediately north of the city centre. It is larger than London's famous Hyde Park and Hampstead Heath put together and the freemen of the city have the right to graze cattle on it. The right incidentally extends to the pitch of St. James' Park, Newcastle United Football Club's ground, though this is not exercised, although the Freemen do collect rent for the loss of privilege. Honorary freemen include Bob Geldof, King Harald V of Norway, Bobby Robson, Alan Shearer, the late Nelson Mandela and the Royal Shakespeare Company. The Hoppings funfair, said to be the largest travelling funfair in Europe, is held here annually in June.\",\n",
       " 'question': 'Where is the Hoppings funfair held?',\n",
       " 'answers': {'text': ['Town Moor'], 'answer_start': [40]},\n",
       " 'metadata': {'split': 'validation', 'model_in_the_loop': 'Combined'}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8bdccc01-fbef-47db-8833-aaaf1c5f2df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af7d0a992424f93858370fb53d732b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb370aeacd6943a581a57e89ecf0b4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c6c73255c54ab794a16e1af65666af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d63460593d74167afbd29cc138b3279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"adversarial_qa\", 'adversarialQA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ex():\n",
    "    item = dataset.shuffle()[\"validation\"][0]\n",
    "    question = item[\"question\"]\n",
    "    context = item[\"context\"]\n",
    "    answer = item[\"answers\"][\"text\"][0]\n",
    "    print(f\"Based on the context below, answer the question. You should just reply with answer string, not a whole sentence.\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer: \")\n",
    "    print()\n",
    "    print()\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context below, answer the question. You should just reply with answer string, not a whole sentence.\n",
      "Context:Genghis Khan is regarded as one of the prominent leaders in Mongolia's history. He is responsible for the emergence of the Mongols as a political and ethnic identity because there was no unified identity between the tribes that had cultural similarity. He reinforced many Mongol traditions and provided stability and unity during a time of almost endemic warfare between tribes. He is also given credit for the introduction of the traditional Mongolian script and the creation of the Ikh Zasag (Great Administration), the first written Mongolian law. \"Ikh Zasag law adopted during Genghis Khan’s time in Mongolia had points to punish illegal matters related to corruption and bribery very heavily,\" Mongolian President Tsakhiagiin Elbegdorj noted. President Elbegdorj sees Genghis Khan as a leader from whom to learn for anti-corruption efforts as Genghis Khan sought equal protection under the law for all citizens regardless of status or wealth. \"Chinggis (Genghis Khan)...was a man who deeply realized that the justice begins and consolidates with the equality of law, and not with the distinctions between people. He was a man who knew that the good laws and rules lived longer than fancy palaces,\" Elbegdorj said in his speech on the 850th anniversary of Chinggis Khaan's birth. In summary, Mongolians see him as the fundamental figure in the founding of the Mongol Empire and therefore the basis for Mongolia as a country.\n",
      "\n",
      "Question: What was the Ikh Zasag?\n",
      "Answer:\n",
      "\n",
      "\n",
      "Answer: the first written Mongolian law\n"
     ]
    }
   ],
   "source": [
    "make_ex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  6 16:41:38 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   60C    P0             252W / 400W |  75000MiB / 81920MiB |     87%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   32C    P0              60W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   71C    P0             174W / 400W |  12917MiB / 81920MiB |     98%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   62C    P0             306W / 400W |   8391MiB / 81920MiB |     98%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    939160      C   ...e/jeongseokoh/miniconda3/bin/python    34252MiB |\n",
      "|    0   N/A  N/A    944773      C   ...park/miniconda3/envs/exp/bin/python    40730MiB |\n",
      "|    2   N/A  N/A    952319      C   python                                    12904MiB |\n",
      "|    3   N/A  N/A    952319      C   python                                     8378MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "               475       gpu     test jeongseo  R    2:28:13      1 gpu-1\n",
      "               477       gpu    bienc  yikyung  R    1:34:18      1 gpu-1\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
