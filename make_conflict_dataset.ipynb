{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abc826b-a3cc-47f6-8e26-b1bd7c1bc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from src.classes.qadataset import QADataset\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple, Union\n",
    "import re\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486c8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_dataset(\"Seongill/Trivia_missing_5_small\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6365492-b4ed-41bf-813f-69d690686932",
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_dataset = pd.DataFrame(load_dataset(\"Seongill/Trivia_missing_5\", split=\"train\"))\n",
    "#nq_dataset= pd.DataFrame(load_dataset(\"Seongill/nq\", split=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5155e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb95d4dc3d5499e8464f9692f2ef431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = pd.DataFrame(load_dataset(\"Seongill/Trivia_missing_10\", split=\"train\"))\n",
    "\n",
    "subset_questions = trivia_dataset.question.tolist()\n",
    "sub_dataset = dataset[dataset[\"question\"].isin(subset_questions)]\n",
    "print(len(sub_dataset))\n",
    "\n",
    "q_to_new_ctxs = dict()\n",
    "for i, row in tqdm(sub_dataset.iterrows()):\n",
    "    q_to_new_ctxs[row[\"question\"]] = row[\"ctxs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 87622 examples from /data/seongil/datasets/normalized/TriviaTrain.jsonl.gz\n",
      "Read 11313 examples from /data/seongil/datasets/normalized/TriviaTest.jsonl.gz\n",
      "Answer Type: PERSON | Size of Group: 22369\n",
      "Answer Type: LOCATION | Size of Group: 10777\n",
      "Answer Type: None | Size of Group: 42978\n",
      "Answer Type: ORGANIZATION | Size of Group: 8237\n",
      "Answer Type: NUMERIC | Size of Group: 2070\n",
      "Answer Type: DATE | Size of Group: 1191\n"
     ]
    }
   ],
   "source": [
    "train, test = QADataset.load(\"TriviaTrain\"), QADataset.load(\"TriviaTest\")\n",
    "res = dict()\n",
    "grouped_examples = defaultdict(list)\n",
    "cnt = 0\n",
    "for ex in train.examples:\n",
    "    grouped_examples[ex.get_example_answer_type()].append(ex)\n",
    "for group, ex_list in grouped_examples.items():\n",
    "    print(f\"Answer Type: {group} | Size of Group: {len(ex_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66cf85f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19b74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of None: 42978\n",
      "Answer Type: PERSON | Size of Group: 22369\n",
      "Answer Type: LOCATION | Size of Group: 10777\n",
      "Answer Type: None | Size of Group: 42978\n",
      "Answer Type: ORGANIZATION | Size of Group: 8237\n",
      "Answer Type: NUMERIC | Size of Group: 2070\n",
      "Answer Type: DATE | Size of Group: 1191\n",
      "--------------------------------------------------\n",
      "Answer Type: PERSON | Size of Group: 78587\n",
      "Answer Type: LOCATION | Size of Group: 26150\n",
      "Answer Type: ORGANIZATION | Size of Group: 41415\n",
      "Answer Type: NUMERIC | Size of Group: 1270\n",
      "Answer Type: DATE | Size of Group: 2354\n",
      "Length of None: 42978\n",
      "Total Mentions: 149724\n",
      "149724\n"
     ]
    }
   ],
   "source": [
    "unique_mentions = []\n",
    "mention_to_ner = dict()\n",
    "ent_to_answer_unique = defaultdict(list)\n",
    "\n",
    "for k,v in grouped_examples.items():\n",
    "    if k == None:\n",
    "        print(\"Length of None:\", len(v))\n",
    "        continue\n",
    "    else:\n",
    "        entities = []\n",
    "        for ex in v:    \n",
    "            answers = [d.text for d in ex.gold_answers]\n",
    "            entities.extend(answers)\n",
    "        entities = list(set(entities))\n",
    "        ent_to_answer_unique[k] = entities\n",
    "for group, ex_list in grouped_examples.items():\n",
    "    print(f\"Answer Type: {group} | Size of Group: {len(ex_list)}\")\n",
    "print(\"-\"*50)\n",
    "for group, ex_list in ent_to_answer_unique.items():\n",
    "    print(f\"Answer Type: {group} | Size of Group: {len(ex_list)}\")\n",
    "\n",
    "for k,v in grouped_examples.items():\n",
    "    if k == None:\n",
    "        print(\"Length of None:\", len(v))\n",
    "        continue\n",
    "    else:\n",
    "        entities = []\n",
    "        for ex in v:    \n",
    "            answers = [d.text for d in ex.gold_answers]\n",
    "            entities.extend(answers)\n",
    "        entities = list(set(entities))\n",
    "        unique_mentions.extend(entities)\n",
    "        for ent in entities:\n",
    "            mention_to_ner[ent] = k\n",
    "unique_mentions = list(set(unique_mentions))\n",
    "print(\"Total Mentions:\", len(unique_mentions))\n",
    "print(len(mention_to_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd1136b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b4ff80e286467ba895eef6b73bb5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "149724"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_type_vector_map = defaultdict(list)\n",
    "embeds = []\n",
    "for i in tqdm(range(0, len(unique_mentions), 4000)):\n",
    "    batch = unique_mentions[i:i+4000]\n",
    "    for mention, _doc in zip(batch, list(nlp.pipe(batch))):\n",
    "        embeds.append(_doc.vector)\n",
    "        ent_type_vector_map[mention_to_ner[mention]].append(_doc.vector)\n",
    "len(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8370622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 11313\n",
      "after: 3771\n"
     ]
    }
   ],
   "source": [
    "entity_vector_map = dict()\n",
    "for mention, vec in zip(unique_mentions, embeds):\n",
    "    entity_vector_map[mention] = vec\n",
    "\n",
    "subset_questions = trivia_dataset[\"question\"].tolist()\n",
    "print(\"before:\", len(test.examples))\n",
    "test.examples = list(filter(lambda x: x.query in subset_questions, test.examples))\n",
    "print(\"after:\", len(test.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0413991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050324ad77e84202a2068ffecd86fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = []\n",
    "for ex in test.examples:\n",
    "    ans.extend([d.text for d in ex.gold_answers])\n",
    "ans = list(set(ans))\n",
    "for i in tqdm(range(0, len(ans), 4000)):\n",
    "    batch = ans[i:i+4000]\n",
    "    for mention, _doc in zip(batch, list(nlp.pipe(batch))):\n",
    "        entity_vector_map[mention] = _doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7104b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "def check_cosine_similarity(arr1, arr2, thres):\n",
    "    cosine_similarity = np.dot(arr1, arr2) / (norm(arr1) * norm(arr2))\n",
    "    return (cosine_similarity <= thres) and (cosine_similarity >= 0.6)\n",
    "    \n",
    "def find_non_identical_random_entity(ent_type, origin: Union[str, List[str]], thres: float=0.8):\n",
    "    subsets = ent_to_answer_unique[ent_type]\n",
    "    #random_entities = random.sample(subsets, 100 if len(subsets)>100 else len(subsets))\n",
    "    for entity in subsets:#random_entities:\n",
    "        origin = origin if not isinstance(origin, list) else random.choice(origin)\n",
    "        origin_vec, entity_vec = entity_vector_map[origin], entity_vector_map[entity]\n",
    "        if check_cosine_similarity(origin_vec, entity_vec, thres):\n",
    "            return entity\n",
    "    return random.choice(subsets)\n",
    "\n",
    "def find_random_entity(ent_type, origin: Union[str, List[str]]):\n",
    "    subsets = ent_to_answer_unique[ent_type]\n",
    "    random_entity = random.choice(subsets)\n",
    "    if isinstance(origin, list):\n",
    "        while random_entity in origin:\n",
    "            random_entity = random.choice(subsets)\n",
    "    else:\n",
    "        while random_entity == origin:\n",
    "            random_entity = random.choice(subsets)\n",
    "    return random_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bb85aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a95fe97ca34bca8a6e710954dfa59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3274570/2366755019.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cosine_similarity = np.dot(arr1, arr2) / (norm(arr1) * norm(arr2))\n"
     ]
    }
   ],
   "source": [
    "question_map = dict()\n",
    "random_subs, similar_subs, questions, gold_answers, ent_types = [], [], [], [], []\n",
    "for ex in tqdm(test.examples):\n",
    "    query, answers, ent_type = ex.query, [d.text for d in ex.gold_answers], ex.get_example_answer_type()\n",
    "    questions.append(query)\n",
    "    gold_answers.append(answers)\n",
    "    if ent_type:\n",
    "        random_sub = find_random_entity(ent_type, answers)\n",
    "        similar_sub = find_non_identical_random_entity(ent_type, answers)\n",
    "    else:\n",
    "        random_sub, similar_sub = None, None\n",
    "    random_subs.append(random_sub)\n",
    "    similar_subs.append(similar_sub)\n",
    "    ent_types.append(ent_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec50138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"question\": questions, \"gold_answers\": gold_answers, \"random_sub\": random_subs, \"similar_sub\": similar_subs, \"ent_type\": ent_types})\n",
    "trivia_dataset = trivia_dataset.merge(df, on=\"question\", how=\"left\")\n",
    "trivia_dataset.drop(columns=[\"gold_answers\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "104d7985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb1b5a590724544b9309df88ae3ec94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Answers in Context: 2.1349774595597983\n",
      "Average Answers: 13.550251922566959\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "ans_count, in_context_count = [], []\n",
    "for i, row in tqdm(trivia_dataset.iterrows(), total=len(trivia_dataset)):\n",
    "    answers_in_context = []\n",
    "    ans_count.append(len(row[\"answers\"]))\n",
    "    for answer in row[\"answers\"]:\n",
    "        for ctx in row[\"ctxs\"]:\n",
    "            if answer in ctx[\"text\"]:\n",
    "                answers_in_context.append(answer)\n",
    "    in_context_count.append(len(answers_in_context))\n",
    "    res.append(answers_in_context)\n",
    "print(\"Average Answers in Context:\", np.mean(in_context_count))\n",
    "print(\"Average Answers:\", np.mean(ans_count))\n",
    "trivia_dataset[\"answers_in_context\"] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c60217",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in tqdm(test.examples):\n",
    "    answer_type = ex.get_example_answer_type()\n",
    "    if not answer_type:\n",
    "        res[ex.query] = None\n",
    "        continue\n",
    "    item = random.choice(grouped_examples[answer_type])\n",
    "    while item == ex:\n",
    "        item = random.choice(grouped_examples[answer_type])\n",
    "    res[ex.query] = item.gold_answers[0].text\n",
    "    cnt += 1\n",
    "trivia_dataset[\"substitute\"] = trivia_dataset[\"question\"].apply(lambda x: res[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc92105",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for row in tqdm(df.iterrows()):\n",
    "    res.append(find_non_identical_random_entity(mention_to_ner[row[1][\"answer\"]], row[1][\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer_in_context(answer_text: str, context: str):\n",
    "    if isinstance(context, str):\n",
    "        context_spans = [\n",
    "            (m.start(), m.end())\n",
    "            for m in re.finditer(re.escape(answer_text.lower()), context.lower())\n",
    "        ]\n",
    "        return context_spans\n",
    "    else:\n",
    "        return [\"\"]\n",
    "def update_context_with_substitution_string(\n",
    "    context: str, originals:List[str], substitution: str, replace_every_string=True\n",
    ") -> str:\n",
    "    replace_spans = []\n",
    "    for orig_answer in originals:\n",
    "        replace_spans.extend(find_answer_in_context(orig_answer, context))\n",
    "    replace_strs = set([context[span[0] : span[1]] for span in replace_spans])\n",
    "    for replace_str in replace_strs:\n",
    "        context = context.replace(replace_str, substitution)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Conflict: 1426\n",
      "Average of hasanswer: 3.3044285335454786\n",
      "Averaged Number of Replaced Contexts: 2.3043478260869565\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f6d8068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ctxs</th>\n",
       "      <th>has_answer</th>\n",
       "      <th>random_sub</th>\n",
       "      <th>similar_sub</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>answers_in_context</th>\n",
       "      <th>num_has_answer_ctx</th>\n",
       "      <th>answer_candidates</th>\n",
       "      <th>valid_answer_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of dance shoe has a specially harden...</td>\n",
       "      <td>[Tap Dance, Tapdance, Soft shoe dance, Tap dan...</td>\n",
       "      <td>[{'hasanswer': False, 'id': '371852', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What type of dance shoe has a specially harden...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [Tap Dance, Tapdance, Soft shoe dance, Tap dan...   \n",
       "\n",
       "                                                ctxs  has_answer random_sub  \\\n",
       "0  [{'hasanswer': False, 'id': '371852', 'score':...        True       None   \n",
       "\n",
       "  similar_sub ent_type answers_in_context  num_has_answer_ctx  \\\n",
       "0        None     None                 []                   1   \n",
       "\n",
       "   answer_candidates  valid_answer_candidates  \n",
       "0                  0                        0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trivia_dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "378e9fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Conflict: 699\n",
      "Average of hasanswer: 1.9345001325908247\n",
      "Averaged Number of Replaced Contexts: 1.251788268955651\n"
     ]
    }
   ],
   "source": [
    "is_conflict, num_replace, random_new_ctxs, num_ans = [], [], [], []\n",
    "TOPK = 5\n",
    "half = False\n",
    "buffer = False\n",
    "for row_ in trivia_dataset.iterrows():\n",
    "    row = row_[1]\n",
    "    ctxs = deepcopy(row[\"ctxs\"][:TOPK])\n",
    "    if row[\"random_sub\"] is None:\n",
    "        is_conflict.append(False)\n",
    "        num_replace.append(0)\n",
    "        random_new_ctxs.append(ctxs)\n",
    "    else:\n",
    "        num_answers = row[\"valid_answer_candidates\"]\n",
    "        if num_answers > 1:\n",
    "            if buffer and half:\n",
    "                is_conflict.append(False)\n",
    "                num_replace.append(0)\n",
    "                random_new_ctxs.append(ctxs)\n",
    "                buffer = False\n",
    "                continue\n",
    "            buffer = True\n",
    "            random.seed(42)\n",
    "            substitute = row[\"random_sub\"]\n",
    "            ctxs_answer_in = [idx for idx, d in enumerate(ctxs) if d[\"nli_has_answer\"]]\n",
    "            replace_num = random.randint(1, num_answers-1)\n",
    "            sampled_idx = random.sample(ctxs_answer_in, replace_num)\n",
    "            for idx, ctx in enumerate(ctxs):\n",
    "                if idx in sampled_idx:\n",
    "                    ctxs[idx][\"text\"] = update_context_with_substitution_string(ctxs[idx][\"text\"], row[\"answers\"], substitute)\n",
    "            random_new_ctxs.append(ctxs)\n",
    "            is_conflict.append(True)\n",
    "            num_replace.append(replace_num)\n",
    "        else:\n",
    "            is_conflict.append(False)\n",
    "            num_replace.append(0)\n",
    "            random_new_ctxs.append(ctxs)\n",
    "print(f\"Number Conflict: {sum(is_conflict)}\")\n",
    "print(f\"Average of hasanswer: {np.mean([sum([int(d['hasanswer']) for d in ctxs]) for ctxs in random_new_ctxs])}\")\n",
    "print(f\"Averaged Number of Replaced Contexts: {np.mean([n for n in num_replace if n > 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94de0cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Conflict: 699\n",
      "Average of hasanswer: 1.9345001325908247\n",
      "Averaged Number of Replaced Contexts: 1.251788268955651\n"
     ]
    }
   ],
   "source": [
    "is_conflict, num_replace, similar_new_ctxs, num_ans = [], [], [], []\n",
    "TOPK = 5\n",
    "half = False\n",
    "buffer = False\n",
    "for row_ in trivia_dataset.iterrows():\n",
    "    row = row_[1]\n",
    "    ctxs = deepcopy(row[\"ctxs\"][:TOPK])\n",
    "    if row[\"random_sub\"] is None:\n",
    "        is_conflict.append(False)\n",
    "        num_replace.append(0)\n",
    "        similar_new_ctxs.append(ctxs)\n",
    "    else:\n",
    "        num_answers = row[\"valid_answer_candidates\"]\n",
    "        if num_answers > 1:\n",
    "            if buffer and half:\n",
    "                is_conflict.append(False)\n",
    "                num_replace.append(0)\n",
    "                similar_new_ctxs.append(ctxs)\n",
    "                buffer = False\n",
    "                continue\n",
    "            buffer = True\n",
    "            random.seed(42)\n",
    "            substitute = row[\"random_sub\"]\n",
    "            ctxs_answer_in = [idx for idx, d in enumerate(ctxs) if d[\"nli_has_answer\"] and d[\"hasanswer\"]]\n",
    "            replace_num = random.randint(1, num_answers-1)\n",
    "            sampled_idx = random.sample(ctxs_answer_in, replace_num)\n",
    "            for idx, ctx in enumerate(ctxs):\n",
    "                if idx in sampled_idx:\n",
    "                    ctxs[idx][\"text\"] = update_context_with_substitution_string(ctxs[idx][\"text\"], row[\"answers\"], substitute)\n",
    "            similar_new_ctxs.append(ctxs)\n",
    "            is_conflict.append(True)\n",
    "            num_replace.append(replace_num)\n",
    "        else:\n",
    "            is_conflict.append(False)\n",
    "            num_replace.append(0)\n",
    "            similar_new_ctxs.append(ctxs)\n",
    "print(f\"Number Conflict: {sum(is_conflict)}\")\n",
    "print(f\"Average of hasanswer: {np.mean([sum([int(d['hasanswer']) for d in ctxs]) for ctxs in similar_new_ctxs])}\")\n",
    "print(f\"Averaged Number of Replaced Contexts: {np.mean([n for n in num_replace if n > 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93053aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = trivia_dataset.copy()\n",
    "new_df = new_df.drop(columns=[\"ctxs\",\"has_answer\"], axis=1)\n",
    "new_df[\"random_substituted_ctxs\"] = random_new_ctxs\n",
    "new_df[\"similar_substituted_ctxs\"] = similar_new_ctxs\n",
    "new_df[\"is_conflict\"] = is_conflict\n",
    "new_df[\"num_replace\"] = num_replace\n",
    "name = f\"TriviaQA_conflict_{TOPK}_{'full' if not half else 'half'}_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24fdb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(columns=[\"answers_in_context\",\"num_has_answer_ctx\", \"answer_candidates\",\"valid_answer_candidates\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1595dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"temp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[(new_df.is_conflict) & (new_df.num_answer > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = new_df.sample(1000, random_state=42)\n",
    "sample_df.is_conflict.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "efd246ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.drop([\"random_sub\", \"similar_sub\", \"ent_type\",\n",
    "                \"answers_in_context\", \"num_has_answer_ctx\",\n",
    "                \"answer_candidates\", \"valid_answer_candidates\",\n",
    "                \"random_substituted_ctxs\"], axis=1, inplace=True)\n",
    "sample_df[\"ctxs\"] = sample_df[\"similar_substituted_ctxs\"]\n",
    "sample_df.drop(\"similar_substituted_ctxs\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e06de9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e5bd198da6499cab6449d317842d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49708e37e2e48b78305f27720bfdca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset.from_pandas(sample_df).push_to_hub(\"TriviaQA_conflict_5_small_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d09351eed1424ca62d5ca44e68c32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12d954a4ae24b80bfc4092c10739d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset.from_pandas(new_df).push_to_hub(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1881"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3610-1729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = \"cross-encoder/qnli-electra-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(nli_model).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(nli_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "404ddbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c980afda5f5448f8b2d6baedfa0961e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "res = []\n",
    "for i, row in tqdm(trivia_dataset.iterrows(), total=len(trivia_dataset)):\n",
    "    query = row[\"question\"]\n",
    "    ctxs = [ctx[\"text\"] for ctx in row[\"ctxs\"]]\n",
    "    features = tokenizer([query]*len(ctxs), ctxs, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        scores = torch.nn.functional.sigmoid(model(**features).logits).detach().cpu().numpy()\n",
    "        idxs = np.where(scores > 0.5)[0]\n",
    "        for j, ctx in enumerate(row[\"ctxs\"]):\n",
    "            if j in idxs:\n",
    "                ctx[\"nli_has_answer\"] = True\n",
    "            else:\n",
    "                ctx[\"nli_has_answer\"] = False\n",
    "        res.append(idxs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d1ab7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ctxs</th>\n",
       "      <th>has_answer</th>\n",
       "      <th>random_sub</th>\n",
       "      <th>similar_sub</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>answers_in_context</th>\n",
       "      <th>num_has_answer_ctx</th>\n",
       "      <th>answer_candidates</th>\n",
       "      <th>valid_answer_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In the Blandings Castle stories by P G Wodehou...</td>\n",
       "      <td>[Chazer, Piggeh, Sus (genus), Pig, Pigs, üê∑, üêñ, üêΩ]</td>\n",
       "      <td>[{'hasanswer': True, 'id': '4601349', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Pig, Pig, Pig, Pigs, Pigs]</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The voice of Bugs Bunny, Daffy Duck, Porky Pig...</td>\n",
       "      <td>[The Man of a Thousand Voices, Sy, the Little ...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '549983', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>PAUL STEPHENSON</td>\n",
       "      <td>Mel C.</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Melvin Jerome Blanc, Melvin Jerome Blanc, Mel...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ecuador has a border with Peru and which other...</td>\n",
       "      <td>[Rep√∫blica de Colombia, Etymology of Colombia,...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '119939', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Gelibolu Peninsula</td>\n",
       "      <td>Garden Creek, New Brunswick</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>[Colombia, Colombia, Colombia, Colombia]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What was the first national park in the USA</td>\n",
       "      <td>[Yellowstone National Park Archives, Yellowsto...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '289806', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Punjab (Province)</td>\n",
       "      <td>Kentucky River</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>[Yellowstone, Yellowstone, Yellowstone, Yellow...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Who designed the Queen's wedding dress?</td>\n",
       "      <td>[Norman Bishop Hartnell, Norman Hartnell, Sir ...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '15572631', 'score'...</td>\n",
       "      <td>True</td>\n",
       "      <td>The Pied Piper of R&amp;B</td>\n",
       "      <td>John McEnroe</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Norman Hartnell, Norman Hartnell, Norman Hart...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>What was the Troggs most famous hit?</td>\n",
       "      <td>[Wild Thing (song), Wild Thing, Wild Thing (di...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '1615929', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Wild Thing, Wild Thing, Wild Thing, Wild Thing]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>Cholecystectomy is the surgical removal of whi...</td>\n",
       "      <td>[Galblader, Gal blader, Corpus vesicae biliari...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '2944151', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Cholecyst]</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>What animal is the Scandinavian Christmas Julb...</td>\n",
       "      <td>[Nanny goat, Dairy goat, Kid (goat), Nanny Goa...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '6670299', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>Lady Chatterley's Lover (book)</td>\n",
       "      <td>Kung Zhu</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Goat, Goat, Goat, Goats]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>Also known as honey wine, what is the name of ...</td>\n",
       "      <td>[Hydromel, Metheglins, Pyment, Rhodomel, Honey...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '454926', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Mead]</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>What is the name for a natural satellite that ...</td>\n",
       "      <td>[Sol 3a, Moon-like, Mass of the Moon, Solar an...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '2837334', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Moon, Moon, Moon, Moon, Moon, Earth's Moon]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "7     In the Blandings Castle stories by P G Wodehou...   \n",
       "11    The voice of Bugs Bunny, Daffy Duck, Porky Pig...   \n",
       "18    Ecuador has a border with Peru and which other...   \n",
       "19          What was the first national park in the USA   \n",
       "25              Who designed the Queen's wedding dress?   \n",
       "...                                                 ...   \n",
       "3751               What was the Troggs most famous hit?   \n",
       "3752  Cholecystectomy is the surgical removal of whi...   \n",
       "3756  What animal is the Scandinavian Christmas Julb...   \n",
       "3761  Also known as honey wine, what is the name of ...   \n",
       "3770  What is the name for a natural satellite that ...   \n",
       "\n",
       "                                                answers  \\\n",
       "7     [Chazer, Piggeh, Sus (genus), Pig, Pigs, üê∑, üêñ, üêΩ]   \n",
       "11    [The Man of a Thousand Voices, Sy, the Little ...   \n",
       "18    [Rep√∫blica de Colombia, Etymology of Colombia,...   \n",
       "19    [Yellowstone National Park Archives, Yellowsto...   \n",
       "25    [Norman Bishop Hartnell, Norman Hartnell, Sir ...   \n",
       "...                                                 ...   \n",
       "3751  [Wild Thing (song), Wild Thing, Wild Thing (di...   \n",
       "3752  [Galblader, Gal blader, Corpus vesicae biliari...   \n",
       "3756  [Nanny goat, Dairy goat, Kid (goat), Nanny Goa...   \n",
       "3761  [Hydromel, Metheglins, Pyment, Rhodomel, Honey...   \n",
       "3770  [Sol 3a, Moon-like, Mass of the Moon, Solar an...   \n",
       "\n",
       "                                                   ctxs  has_answer  \\\n",
       "7     [{'hasanswer': True, 'id': '4601349', 'score':...        True   \n",
       "11    [{'hasanswer': True, 'id': '549983', 'score': ...        True   \n",
       "18    [{'hasanswer': True, 'id': '119939', 'score': ...        True   \n",
       "19    [{'hasanswer': True, 'id': '289806', 'score': ...        True   \n",
       "25    [{'hasanswer': True, 'id': '15572631', 'score'...        True   \n",
       "...                                                 ...         ...   \n",
       "3751  [{'hasanswer': True, 'id': '1615929', 'score':...        True   \n",
       "3752  [{'hasanswer': True, 'id': '2944151', 'score':...        True   \n",
       "3756  [{'hasanswer': True, 'id': '6670299', 'score':...        True   \n",
       "3761  [{'hasanswer': True, 'id': '454926', 'score': ...        True   \n",
       "3770  [{'hasanswer': True, 'id': '2837334', 'score':...        True   \n",
       "\n",
       "                          random_sub                  similar_sub  ent_type  \\\n",
       "7                               None                         None      None   \n",
       "11                   PAUL STEPHENSON                       Mel C.    PERSON   \n",
       "18                Gelibolu Peninsula  Garden Creek, New Brunswick  LOCATION   \n",
       "19                 Punjab (Province)               Kentucky River  LOCATION   \n",
       "25             The Pied Piper of R&B                 John McEnroe    PERSON   \n",
       "...                              ...                          ...       ...   \n",
       "3751                            None                         None      None   \n",
       "3752                            None                         None      None   \n",
       "3756  Lady Chatterley's Lover (book)                     Kung Zhu    PERSON   \n",
       "3761                            None                         None      None   \n",
       "3770                            None                         None      None   \n",
       "\n",
       "                                     answers_in_context  num_has_answer_ctx  \\\n",
       "7                           [Pig, Pig, Pig, Pigs, Pigs]                   5   \n",
       "11    [Melvin Jerome Blanc, Melvin Jerome Blanc, Mel...                   5   \n",
       "18             [Colombia, Colombia, Colombia, Colombia]                   4   \n",
       "19    [Yellowstone, Yellowstone, Yellowstone, Yellow...                   5   \n",
       "25    [Norman Hartnell, Norman Hartnell, Norman Hart...                   3   \n",
       "...                                                 ...                 ...   \n",
       "3751   [Wild Thing, Wild Thing, Wild Thing, Wild Thing]                   4   \n",
       "3752                                        [Cholecyst]                   5   \n",
       "3756                          [Goat, Goat, Goat, Goats]                   4   \n",
       "3761                                             [Mead]                   5   \n",
       "3770       [Moon, Moon, Moon, Moon, Moon, Earth's Moon]                   5   \n",
       "\n",
       "      answer_candidates  valid_answer_candidates  \n",
       "7                     4                        4  \n",
       "11                    4                        4  \n",
       "18                    3                        3  \n",
       "19                    4                        4  \n",
       "25                    4                        3  \n",
       "...                 ...                      ...  \n",
       "3751                  4                        4  \n",
       "3752                  5                        5  \n",
       "3756                  3                        3  \n",
       "3761                  4                        4  \n",
       "3770                  3                        3  \n",
       "\n",
       "[486 rows x 11 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trivia_dataset[\"num_has_answer_ctx\"] = trivia_dataset[\"ctxs\"].apply(lambda x: sum([d[\"hasanswer\"] for d in x]))\n",
    "trivia_dataset[\"answer_candidates\"] = res\n",
    "trivia_dataset[\"valid_answer_candidates\"] = trivia_dataset[\"ctxs\"].apply(lambda x: sum([True if d[\"hasanswer\"] and d[\"nli_has_answer\"] else False for d in x]))\n",
    "trivia_dataset[(trivia_dataset.valid_answer_candidates > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8656f6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ctxs</th>\n",
       "      <th>has_answer</th>\n",
       "      <th>random_sub</th>\n",
       "      <th>similar_sub</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>answers_in_context</th>\n",
       "      <th>num_has_answer_ctx</th>\n",
       "      <th>answer_candidates</th>\n",
       "      <th>valid_answer_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The voice of Bugs Bunny, Daffy Duck, Porky Pig...</td>\n",
       "      <td>[The Man of a Thousand Voices, Sy, the Little ...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '549983', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>PAUL STEPHENSON</td>\n",
       "      <td>Mel C.</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Melvin Jerome Blanc, Melvin Jerome Blanc, Mel...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ecuador has a border with Peru and which other...</td>\n",
       "      <td>[Rep√∫blica de Colombia, Etymology of Colombia,...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '119939', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Gelibolu Peninsula</td>\n",
       "      <td>Garden Creek, New Brunswick</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>[Colombia, Colombia, Colombia, Colombia]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What was the first national park in the USA</td>\n",
       "      <td>[Yellowstone National Park Archives, Yellowsto...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '289806', 'score': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Punjab (Province)</td>\n",
       "      <td>Kentucky River</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>[Yellowstone, Yellowstone, Yellowstone, Yellow...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Which 19th century king sired ten illegitimate...</td>\n",
       "      <td>[William IV (disambiguation), Wilhelm IV, Will...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '1296740', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>Francis Albert Augustus Charles Emmanuel</td>\n",
       "      <td>John McEnroe</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[William IV, William IV, William IV]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Who designed the Queen's wedding dress?</td>\n",
       "      <td>[Norman Bishop Hartnell, Norman Hartnell, Sir ...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '15572631', 'score'...</td>\n",
       "      <td>True</td>\n",
       "      <td>The Pied Piper of R&amp;B</td>\n",
       "      <td>John McEnroe</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Norman Hartnell, Norman Hartnell, Norman Hart...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>In which modern country is the ancient city of...</td>\n",
       "      <td>[Jordan (country), Al-Urdunn, ÿ£ÿ±ÿØŸÜŸë, JOrdan, U...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '7118350', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>C. J. Parker</td>\n",
       "      <td>Gazi Mustafa Kemal Pasha</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Jordan, Jordan, Jordan, Jordan]</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>Opened in 1637, in which city was the first pu...</td>\n",
       "      <td>[Sestiere (Venice), Venice, Venedig, Districts...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '9805483', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>WYE</td>\n",
       "      <td>Italian capital</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>[Venice, Venice, Venice, Venice]</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>Which newsreader famously sat on a Lesbian whe...</td>\n",
       "      <td>[Nicholas Newton Henshall Witchell, Nick Witch...</td>\n",
       "      <td>[{'hasanswer': False, 'id': '6336484', 'score'...</td>\n",
       "      <td>True</td>\n",
       "      <td>(Norman) Foster</td>\n",
       "      <td>Arnold Daniel Palmer</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Nicholas Witchell, Nicholas Witchell]</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>What animal is the Scandinavian Christmas Julb...</td>\n",
       "      <td>[Nanny goat, Dairy goat, Kid (goat), Nanny Goa...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '6670299', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>Lady Chatterley's Lover (book)</td>\n",
       "      <td>Kung Zhu</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[Goat, Goat, Goat, Goats]</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>Which Championship Football League club is bas...</td>\n",
       "      <td>[Leicester Fosse FC, Leicetser City F.C., Leic...</td>\n",
       "      <td>[{'hasanswer': True, 'id': '4498355', 'score':...</td>\n",
       "      <td>True</td>\n",
       "      <td>Telefonica Group</td>\n",
       "      <td>Leeds United AFC</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[Leicester City Football Club, Leicester Fosse...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "11    The voice of Bugs Bunny, Daffy Duck, Porky Pig...   \n",
       "18    Ecuador has a border with Peru and which other...   \n",
       "19          What was the first national park in the USA   \n",
       "22    Which 19th century king sired ten illegitimate...   \n",
       "25              Who designed the Queen's wedding dress?   \n",
       "...                                                 ...   \n",
       "3732  In which modern country is the ancient city of...   \n",
       "3739  Opened in 1637, in which city was the first pu...   \n",
       "3743  Which newsreader famously sat on a Lesbian whe...   \n",
       "3756  What animal is the Scandinavian Christmas Julb...   \n",
       "3760  Which Championship Football League club is bas...   \n",
       "\n",
       "                                                answers  \\\n",
       "11    [The Man of a Thousand Voices, Sy, the Little ...   \n",
       "18    [Rep√∫blica de Colombia, Etymology of Colombia,...   \n",
       "19    [Yellowstone National Park Archives, Yellowsto...   \n",
       "22    [William IV (disambiguation), Wilhelm IV, Will...   \n",
       "25    [Norman Bishop Hartnell, Norman Hartnell, Sir ...   \n",
       "...                                                 ...   \n",
       "3732  [Jordan (country), Al-Urdunn, ÿ£ÿ±ÿØŸÜŸë, JOrdan, U...   \n",
       "3739  [Sestiere (Venice), Venice, Venedig, Districts...   \n",
       "3743  [Nicholas Newton Henshall Witchell, Nick Witch...   \n",
       "3756  [Nanny goat, Dairy goat, Kid (goat), Nanny Goa...   \n",
       "3760  [Leicester Fosse FC, Leicetser City F.C., Leic...   \n",
       "\n",
       "                                                   ctxs  has_answer  \\\n",
       "11    [{'hasanswer': True, 'id': '549983', 'score': ...        True   \n",
       "18    [{'hasanswer': True, 'id': '119939', 'score': ...        True   \n",
       "19    [{'hasanswer': True, 'id': '289806', 'score': ...        True   \n",
       "22    [{'hasanswer': True, 'id': '1296740', 'score':...        True   \n",
       "25    [{'hasanswer': True, 'id': '15572631', 'score'...        True   \n",
       "...                                                 ...         ...   \n",
       "3732  [{'hasanswer': True, 'id': '7118350', 'score':...        True   \n",
       "3739  [{'hasanswer': True, 'id': '9805483', 'score':...        True   \n",
       "3743  [{'hasanswer': False, 'id': '6336484', 'score'...        True   \n",
       "3756  [{'hasanswer': True, 'id': '6670299', 'score':...        True   \n",
       "3760  [{'hasanswer': True, 'id': '4498355', 'score':...        True   \n",
       "\n",
       "                                    random_sub                  similar_sub  \\\n",
       "11                             PAUL STEPHENSON                       Mel C.   \n",
       "18                          Gelibolu Peninsula  Garden Creek, New Brunswick   \n",
       "19                           Punjab (Province)               Kentucky River   \n",
       "22    Francis Albert Augustus Charles Emmanuel                 John McEnroe   \n",
       "25                       The Pied Piper of R&B                 John McEnroe   \n",
       "...                                        ...                          ...   \n",
       "3732                              C. J. Parker     Gazi Mustafa Kemal Pasha   \n",
       "3739                                       WYE              Italian capital   \n",
       "3743                           (Norman) Foster         Arnold Daniel Palmer   \n",
       "3756            Lady Chatterley's Lover (book)                     Kung Zhu   \n",
       "3760                          Telefonica Group             Leeds United AFC   \n",
       "\n",
       "          ent_type                                 answers_in_context  \\\n",
       "11          PERSON  [Melvin Jerome Blanc, Melvin Jerome Blanc, Mel...   \n",
       "18        LOCATION           [Colombia, Colombia, Colombia, Colombia]   \n",
       "19        LOCATION  [Yellowstone, Yellowstone, Yellowstone, Yellow...   \n",
       "22          PERSON               [William IV, William IV, William IV]   \n",
       "25          PERSON  [Norman Hartnell, Norman Hartnell, Norman Hart...   \n",
       "...            ...                                                ...   \n",
       "3732        PERSON                   [Jordan, Jordan, Jordan, Jordan]   \n",
       "3739      LOCATION                   [Venice, Venice, Venice, Venice]   \n",
       "3743        PERSON             [Nicholas Witchell, Nicholas Witchell]   \n",
       "3756        PERSON                          [Goat, Goat, Goat, Goats]   \n",
       "3760  ORGANIZATION  [Leicester City Football Club, Leicester Fosse...   \n",
       "\n",
       "      num_has_answer_ctx  answer_candidates  valid_answer_candidates  \n",
       "11                     5                  4                        4  \n",
       "18                     4                  3                        3  \n",
       "19                     5                  4                        4  \n",
       "22                     3                  2                        2  \n",
       "25                     3                  4                        3  \n",
       "...                  ...                ...                      ...  \n",
       "3732                   4                  2                        2  \n",
       "3739                   4                  2                        2  \n",
       "3743                   2                  4                        2  \n",
       "3756                   4                  3                        3  \n",
       "3760                   4                  2                        2  \n",
       "\n",
       "[699 rows x 11 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trivia_dataset[(trivia_dataset.valid_answer_candidates > 1) & (trivia_dataset.ent_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f708ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame(load_dataset(\"Seongill/squad_adversarial_thres1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0fc78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'question': 'The Basilica of the Sacred heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'question': 'Where is the headquarters of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'question': 'What is the primary seminary of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'question': 'What is the oldest structure at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'question': 'How many BS level degrees are of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22996</th>\n",
       "      <td>{'question': 'How many scalar equations were f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22997</th>\n",
       "      <td>{'question': 'How many vector equations did He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22998</th>\n",
       "      <td>{'question': 'Who discovered that magnetic and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22999</th>\n",
       "      <td>{'question': 'At what temperature do weak and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23000</th>\n",
       "      <td>{'question': 'What is the law of thermodynamic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23001 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   train\n",
       "0      {'question': 'The Basilica of the Sacred heart...\n",
       "1      {'question': 'Where is the headquarters of the...\n",
       "2      {'question': 'What is the primary seminary of ...\n",
       "3      {'question': 'What is the oldest structure at ...\n",
       "4      {'question': 'How many BS level degrees are of...\n",
       "...                                                  ...\n",
       "22996  {'question': 'How many scalar equations were f...\n",
       "22997  {'question': 'How many vector equations did He...\n",
       "22998  {'question': 'Who discovered that magnetic and...\n",
       "22999  {'question': 'At what temperature do weak and ...\n",
       "23000  {'question': 'What is the law of thermodynamic...\n",
       "\n",
       "[23001 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f067c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
